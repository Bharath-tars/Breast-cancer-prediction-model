# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(y_test, y_pred))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(y_test, y_pred))

print("Confusiuon Matrix:",metrics.confusion_matrix(y_test, y_pred))


// comments 
/* the output looks something like this */
Accuracy: 0.9649122807017544

Precision: 0.9811320754716981

Recall: 0.9629629629629629

Confusiuon Matrix: [[ 61   2]

 [  4 104]]
